# cuda
cuda_index: 0
seed: 42

# path 
lang: zh
json_path: data/dataset/jsons
preprocessed_dir: data/preprocessed
target_dir: data/save

bert-en:
  bert_path: ./models/roberta-base
  cls: '<s>'
  sep: '</s>'
  unk: '<unk>'
  pad: '<pad>'

bert-zh:
  bert_path: ./models/chinese-roberta-wwm-ext
  cls: '[CLS]'
  sep: '[SEP]'
  unk: '[UNK]'
  pad: '[PAD]'

# chinese dataï¼Œadd space to unkown tokens, if you use english data, you need to remove space from unkown tokens
unkown_tokens: ' ğŸ”â€”ğŸ›ğŸ™‰ğŸ™„ğŸ”¨ğŸ†ğŸ†”ğŸ‘ŒğŸ‘€ğŸ¥ºå†–ğŸŒšğŸ™ˆğŸ˜­ğŸğŸ˜…ğŸ’©å°›ç¡Œç³‡ğŸ’°ğŸ´ğŸ™ŠğŸ’¯â­ğŸ¶ğŸŸğŸ™ğŸ˜„ğŸ»ğŸ“¶ğŸ®ğŸºâŒğŸ¤”ğŸğŸ¸ğŸ™ƒğŸ¤£ğŸ†ğŸ˜‚ğŸŒšâ€œâ€â€˜â€™ç°‹è®£åŸˆé«Œç¢œæ…œéƒ¯åš¯ç£™è¯¹äº–è“¥åŸæœŠæé”¨'
# unkown_tokens: 'ğŸ”â€”ğŸ›ğŸ™‰ğŸ™„ğŸ”¨ğŸ†ğŸ†”ğŸ‘ŒğŸ‘€ğŸ¥ºå†–ğŸŒšğŸ™ˆğŸ˜­ğŸğŸ˜…ğŸ’©å°›ç¡Œç³‡ğŸ’°ğŸ´ğŸ™ŠğŸ’¯â­ğŸ¶ğŸŸğŸ™ğŸ˜„ğŸ»ğŸ“¶ğŸ®ğŸºâŒğŸ¤”ğŸğŸ¸ğŸ™ƒğŸ¤£ğŸ†ğŸ˜‚ğŸŒšâ€œâ€â€˜â€™ç°‹è®£åŸˆé«Œç¢œæ…œéƒ¯åš¯ç£™è¯¹äº–è“¥åŸæœŠæé”¨â€“ï¼Œï¼›ã€ã€‚â–'
max_length: 512

# parameter 
epoch_size: 30
batch_size: 1
lr: 1e-3
bert_lr: 1e-5
patience: 10
max_grad_norm: 1.0
warmup_proportion: 0.1
gradient_accumulation_steps: 1
adam_epsilon: 1e-8
warmup_steps: 0
weight_decay: 0.01

inner_dim: 128

# dict 
bio_mode: 'OBIES'
asp_type: 'Aspect'
tgt_type: 'Target'
opi_type: 'Opinion'

polarity_dict:
  O: 0
  positive: 1
  negative: 2
  neutral: 3

entity_dict:
  O: 0
  ENT-Tri: 1
  ENT-Arg: 2
  ENT-Opi: 3

loss_weight:
  ent: 3
  rel: 5
  pol: 7